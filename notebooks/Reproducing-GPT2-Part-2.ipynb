{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df032ef-c61f-48ed-a3d4-92fd85501ae6",
   "metadata": {},
   "source": [
    "# Let’s Reproduce GPT-2 (124M)\n",
    "\n",
    "## Andrej Karpathy GPT Course - Neural Networks: Zero to Hero\n",
    "\n",
    "### Lesson 10 - Let's Reproduce GPT-2 (124M): Section 2 - Optimization\n",
    "\n",
    "+ YouTube video:\n",
    "    + https://www.youtube.com/watch?v=l8pRSuU81PU\n",
    "\n",
    "#####  Jun 9, 2024\n",
    "\n",
    "##### Andrej's Video Comments\n",
    "We reproduce the GPT-2 (124M) from scratch. This video covers the whole process: First we build the GPT-2 network, then we optimize its training to be really fast, then we set up the training run following the GPT-2 and GPT-3 paper and their hyperparameters, then we hit run, and come back the next morning to see our results, and enjoy some amusing model generations. Keep in mind that in some places this video builds on the knowledge from earlier videos in the Zero to Hero Playlist (see my channel). You could also see this video as building my nanoGPT repo, which by the end is about 90% similar.\n",
    "\n",
    "+ Full course details: \n",
    "    + https://karpathy.ai/zero-to-hero.html\n",
    "+ GitHub repository with all the changes in this video as individual commits (build-nanogpt)\n",
    "    + https://github.com/karpathy/build-nanogpt\n",
    "+ GitHub repository of GPT-2 based on the this \"Let's Reproduce GPT-2\" tutorial by user Lxrd-AJ\n",
    "    + https://github.com/Lxrd-AJ/GPT2\n",
    "+ nanoGPT repository\n",
    "    + https://github.com/karpathy/nanoGPT\n",
    "+ LLM.c Repository\n",
    "    + https://github.com/karpathy/llm.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26125617-56cf-41f9-af40-6322b0e4ac39",
   "metadata": {},
   "source": [
    "### YouTube video contents: Section - Optimization\n",
    "\n",
    "The times in milliseconds in brackets at the end of each chapter here show how much each computation step reduced to after adding the optimization improvement, many of which are as simple as adding a single line of code.\n",
    "\n",
    "#### Chapters\n",
    "+ 01:22:18 - SECTION 2: Let’s make it fast. GPUs, mixed precision, (1000ms)\n",
    "+ 01:28:14 - Tensor Cores, timing the code, TF32 precision, (333ms)\n",
    "+ 01:39:38 - float16, gradient scalers, bfloat16, (300ms)\n",
    "+ 01:48:15 - torch.compile, Python overhead, kernel fusion, (130ms)\n",
    "+ 02:00:18 - flash attention, (96ms)\n",
    "+ 02:06:54 - nice/ugly numbers. vocab size 50257 → 50304, (93ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6125c1-6541-4dbf-a29a-bd01ef2eeb83",
   "metadata": {},
   "source": [
    "## Loading the Tiny Shakespeare dataset which we will use to train the model\n",
    "\n",
    "We'll begin with a very small dataset which gets us off the ground and helps with debugging. Once we've ironed out the kinds with the small dataset we can load something bigger!\n",
    "+ The tiny Shakespeare dataset is available from `https://github.com/karpathy/ng-video-lecture`\n",
    "\n",
    "I already downloaded the dataset to use in the **Let's build GPT** tutorial. It can be imported like this:\n",
    "```\n",
    "import os\n",
    "file_path = r\"07 - Let's Build GPT (nanoGPT)\\input.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "```\n",
    "The tokenizer compression ratio is roughly 3 to 1 so the first 1000 characters is roughly 300 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af77218b-1d91-4b82-beba-7609aea1c20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tiny Shakespeare dataset loaded.\n",
      "Length of dataset in characters: 1115394 \n",
      "\n",
      "First 100 characters in Tiny Shakespeare dataset:\n",
      "\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "Sample tokens:\n",
      " [5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13] \n",
      "\n",
      "Input tensor as four rows of six tokens:\n",
      " tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
      "        [ 5120,   597,  2252,    11,  3285,   502],\n",
      "        [ 2740,    13,   198,   198,  3237,    25],\n",
      "        [  198,  5248,   461,    11,  2740,    13]]) \n",
      "\n",
      "Labels tensor as four rows of six tokens (inputs shifted 1 to the right):\n",
      " tensor([[22307,    25,   198,  8421,   356,  5120],\n",
      "        [  597,  2252,    11,  3285,   502,  2740],\n",
      "        [   13,   198,   198,  3237,    25,   198],\n",
      "        [ 5248,   461,    11,  2740,    13,   198]]) \n",
      "\n",
      "Encoded tokens:\n",
      " tensor([[15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11],\n",
      "        [15496,    11,   314,  1101,   257,  3303,  2746,    11]])\n"
     ]
    }
   ],
   "source": [
    "# Test loading data. We'll rearrange this code below\n",
    "\n",
    "# Load Tiny Shakespeare Dataset\n",
    "import os\n",
    "file_path = r\"07 - Let's Build GPT (nanoGPT)\\input.txt\"\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"\\nTiny Shakespeare dataset loaded.\\nLength of dataset in characters:\", len(text), \"\\n\")\n",
    "data = text[:1000]                                               # First 1,000 characters (which is roughly 300 tokens)\n",
    "print(\"First 100 characters in Tiny Shakespeare dataset:\\n\\n\", data[:100], \"\\n\")\n",
    "\n",
    "# Import tokenizer\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)                                        # Encode the data we extracted from the text earlier\n",
    "print(\"Sample tokens:\\n\", tokens[:24], \"\\n\")                     # For example, token 198 is the newline character\n",
    "\n",
    "# Build input and label tensors from input tokens to push through the model to begin training it.\n",
    "import torch\n",
    "buf = torch.tensor(tokens[:24 + 1])                              # Include extra token to have ground truth for final label\n",
    "x = buf[:-1].view(4,6)                                           # .view gives 2D rearrangement of tokens\n",
    "y = buf[1:].view(4,6)                                            # y loads all but first token (x loaded all but last)\n",
    "print(\"Input tensor as four rows of six tokens:\\n\", x, \"\\n\")\n",
    "print(\"Labels tensor as four rows of six tokens (inputs shifted 1 to the right):\\n\", y, \"\\n\")\n",
    "\n",
    "# Encode starter phrase\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")              # Encode starter phrase as list of 8 integer tokens\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)                  # Form tensor from encoding tokens (8, )\n",
    "tokens = tokens.unsqueeze(0).repeat(5, 1)                        # Replicate the starter encoder tokens several times (5, 8) \n",
    "print (\"Encoded tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3175a582-3219-485c-81b3-2667fe156085",
   "metadata": {},
   "source": [
    "## Second Version: Optimize and train from radomized weights\n",
    "\n",
    "Note that despite using the same Hugging Face model, carefully setting hyperparaters to match and setting the same random seeds the hand-built GPT-2 does not match the output. Andrej tried to hunt down the discrepancy but couldn't find it! However he was able to verify that the model internals (tensors) match by loading getting the same results when loading the Hugging Face model. Basically, something in the `transformers.pipeline` procedure differs from the hand built GPT-2 that Andrej built.\n",
    "\n",
    "Now that we have verified the models are in the same place the idea is to work through the GPT-2 code to improve it. We also want to actually retrain our own model from an initialization of random numbers. By default, when the model is initialized PyTorch does what we want by loading it with random values. In the first version we then imported the Hugging Face trained model and loaded it over the top but in this version we will train the model ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eebd723d-7e6a-4464-94dc-7ad73523d453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION TWO: OPTIMIZED GPT-2 TRAINED FROM RANDOMIZED WEIGHTS\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import inspect\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embed % config.n_head == 0\n",
    "        # key, query, value projections for all heads but batched\n",
    "        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed)          # Project q,k,v in one step\n",
    "        # output projection of the attention layer\n",
    "        self.c_proj = nn.Linear(config.n_embed, config.n_embed)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1                                   # Enable flag for this module\n",
    "        # Regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embed = config.n_embed\n",
    "        # not really a 'bias' - more of a mask. The name is chosen to match OpenAI/HF naming\n",
    "        # buffers are equivalent to States in MATLAB's custom layers\n",
    "        # `bias` is a (1, 1, T, T) tensor, useful for broadcasting\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()     # Batch size, sequence length, embedding dimensionality (e_embed)\n",
    "        # Calculate query, key & values for all heads in batch and move head forward to be the batch\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C is \"number of channels\" (nh * hs)\n",
    "        # In GPT-2 (124M), n_head=12, hs=64 so nh*hs = 768 channels in the transformer\n",
    "        qkv = self.c_attn(x)   # computes query, key & value in parallel. Resulting tensor is (B,T,C) where C = 3 * n_embed\n",
    "        q, k, v = qkv.split(self.n_embed, dim=2)         # Split the BxTx(3*C) matrix into (B,T,C) chunks where C = n_embed\n",
    "        # Apply these multi-head operations efficiently in parallel batches\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, T, nh, hs) --> (B, nh, T, hs)\n",
    "        # Attention (materialises the large (T,T) matrix for all the queries and keys)\n",
    "        \n",
    "        # Original Attention can be replaced by the much more efficient flash attention below:\n",
    "        #att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) # (B, nh, T, hs) @ (B, nh, hs, T) = (B, nh, T, T)\n",
    "        #att = att.masked_fill(self.bias[:,:, :T, :T] == 0, float('-inf'))\n",
    "        #att = F.softmax(att, dim=-1)\n",
    "        #y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) --> (B, nh, T, hs)\n",
    "            \n",
    "        # Alternative faster Flash Attention:\n",
    "        # This is not optimized for ANE as at of 2024, WWDC just happened so it's all beta, and alternative is to re-write \n",
    "        # the attention following Apple's guide at:\n",
    "        #   https://github.com/apple/ml-ane-transformers/blob/main/ane_transformers/reference/multihead_attention.py\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) \n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C) where C = nh * hs\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# Multi-Layer Perceptron (MLP) position-wise fully-connected feedforward network with two linear projections\n",
    "# This is called from within the Block function where it is the final step of each transformer block.\n",
    "# THe GELU activation function is similar to ReLU but with a smoother turn around 0. Historically, the tanh approxomation\n",
    "# version of GELU is used as it is faster than standard GELU but Andrej suspects the speed difference is no longer so large.\n",
    "# Either way, sticking to tanh approximated GELU ensures the model follows the original GPT implementation. BERT uses GELU.\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embed, 4 * config.n_embed)    # Linear layer 1\n",
    "        self.gelu = nn.GELU(approximate='tanh')                      # GELU Activation function with tanh approximation\n",
    "        self.c_proj = nn.Linear(4 * config.n_embed, config.n_embed)  # Linear layer 2\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1                           # Attach flag\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Transformer Blocks building Self Attention and MLP feedforawrd layers in nn.ModuleList(). Each block is h.0, h.1, etc\n",
    "# Note in the initializaion that the Layer Norms are placed before the Self Attention and MLP layers rather than after\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embed)\n",
    "        self.attn = CausalSelfAttention(config)     # Call attention operation\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embed)\n",
    "        self.mlp = MLP(config)                      # Call MLP operation\n",
    "\n",
    "    # Forward pass of what the block computes\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))   # Attention applied after Layer Norm with clean residual pathway for adding back x\n",
    "        x = x + self.mlp(self.ln_2(x))    # Position-wide feedforward applied after Layer Norm with clean residual pathway for x\n",
    "        return x\n",
    "\n",
    "    \n",
    "@dataclass                          # Use decorator to log function calls\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024          # GPT2LMHeadModel uses Max sequence length of 1024, `T`\n",
    "    vocab_size: int = 50257         # GPT2LMHeadModel uses 50257 tokens: 50,000 BPE merges + 256 byte tokens + 1 end of text\n",
    "    n_layer: int = 12               # GPT2LMHeadModel has 12 hidden layers\n",
    "    n_head: int = 12                # GPT2LMHeadModel has 12 heads\n",
    "    n_embed: int = 768              # GPT2LMHeadModel has 768-length embedding\n",
    "\n",
    "# Transformer architecture following the GPT-2 modification of the Attention is All you Need architecture\n",
    "# Use schema and naming used in Hugging Face's GPT2LMHeadModel so we can reuse weights from state.dict()\n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Match \"transformer\" naming used GPT2LMHeadModel's container which holds the NN modules so we can easily reuse weights\n",
    "        # The ModuleDict dictionary allows you to index into the transformer modules using keys\n",
    "        # The transformer blocks are created as a list 'h' (hidden layer) so each block can be easily indexed with an integer\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embed),             # Token embedding weights\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embed),             # Positional embedding weights\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), # Call transformer blocks ('n_layer' times)\n",
    "            ln_f = nn.LayerNorm(config.n_embed)                                # Final layer norm\n",
    "        ))\n",
    "        # Language Model Head: linear module projects from 768 embeddings to 50,000+ possible tokens used in vocabulary\n",
    "        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False) # Final classifier uses no bias in GPT-2   \n",
    "    \n",
    "        # Weight sharing/tying\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        print (\"Weight tying\\n\")\n",
    "        \n",
    "        # Initialize params - `apply` is called for every sub module which runs _init_weights to set std=0.02\n",
    "        self.apply(self._init_weights)\n",
    "        print (\"Initialize weights with standard deviation 0.02\\n\")\n",
    "            \n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):     \n",
    "            std = 0.02                                                       # Default Standard Deviation used by OpenAI\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):                        # If flag is set ...\n",
    "                std *= (2 * self.config.n_layer) ** -0.5                     #   use Xavier initialization instead\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)          # Set standard deviation (OpenAI use 0.02)\n",
    "            if module.bias is not None:                                      # Bias weights initialized to zero\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)         # We'll use 0.02 even though OpenAI use 0.02\n",
    "\n",
    "    # Forward step may include optional targets for the model's training phase\n",
    "    def forward(self, idx, targets=None):                                    # idx has shape (B, T). Tagets are optional\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is {self.config.block_size}\"\n",
    "\n",
    "        # forward the token and position embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)        # Shape (T) - Train on GPU if available\n",
    "        pos_emb = self.transformer.wpe(pos)                                  # position embeddings of shape (T, n_embed)\n",
    "        tok_emb = self.transformer.wte(idx)                                  # token embeddings of shape (B, T, n_embed)\n",
    "        x = tok_emb + pos_emb                                                # Add position & token embeddings with broadcasting\n",
    "\n",
    "        # forward through the transformer blocks\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        # forward through the final LayerNorm and classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)                                             # (B, T, vocab_size)\n",
    "\n",
    "        loss = None                                                          # Default (i.e if we don't load targets)\n",
    "        \n",
    "        # Cross Entropy does not take the 3D BxTbVocab_size tensor input so it must be flattened to 2D\n",
    "        # The -1 as the first argument to .view automatically flattens BxT onto 1D leaving us with 2D data overasll\n",
    "        # The operation also flattens the targets from BxT to a flat 1D tensor.\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))   # Cross entropy loss\n",
    "\n",
    "        return logits, loss                                                  # Return targets and loss\n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        # Load pretrained GPT-2 model weights from hugging Face\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(f\"Loading weights from pretrained gpt: {model_type}\")\n",
    "\n",
    "        # n_layer, n_head and n_embd hyperparameters are determed from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embed=768),   # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embed=1024),  # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embed=1280),  # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embed=1600),  # 774M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257  # Always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024   # Always 1024 for GPT model checkpoints\n",
    "        \n",
    "        # Create from scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()             # Create state_dict for our model and the Hugging Face model\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]  # discard unnecessary buffers (auto regressive masks)\n",
    "        \n",
    "        # init a hugging Face transformer model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # Copy Hugging Face model keys while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]  # ignore these\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]         # ignore these\n",
    "\n",
    "        # The Hugging Face model is from a TensorFlow repository where some weights are transposed so that needs fixing.\n",
    "        # In other words, the OpenAI implementation uses a conv1D rather than a vanilla linear layer which requires a fix.\n",
    "        # Manually make a list of weights that require transposing\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        \n",
    "        # As stated above, the OpenAI checkpoints use a conv1D module, but we only want to use a vanilla linear layer\n",
    "        # This means that we have to transpose weights when we import them if they are in the manually built list above\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy as-is the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "        return model\n",
    "\n",
    "\n",
    "    # FINAL CODE NOT ADDED OR CHECKED YET - Training\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device):\n",
    "        # start with all candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "\n",
    "        # Create optim groups. Any parameter that is 2D will be weight decayed, otherwise no\n",
    "        # i/e all weight tensors in matmuls + embedding decay, all biases and layernorm don't\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device == 'cuda'\n",
    "        print(f\"Using fused AdamW: {use_fused}, inspect check: {fused_available}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ddcd93-aad1-4e9f-b8ed-9a962cdd0388",
   "metadata": {},
   "source": [
    "#### Call the optimized model to test it runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db93c78f-ad62-4a40-92f4-0a4e54f71e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johns\\anaconda3\\envs\\gpt-lab\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pretrained gpt: gpt2\n",
      "Weight tying\n",
      "\n",
      "Initialize weights with standard deviation 0.02\n",
      "\n",
      "didn't crash yay!\n",
      "<class '__main__.GPT'>\n"
     ]
    }
   ],
   "source": [
    "# Call the optimized model to test it runs\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "print (\"didn't crash yay!\")\n",
    "print(GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f7e3d-d72c-42d8-8992-b34f431d4bd0",
   "metadata": {},
   "source": [
    "## Training our GPT-2 model\n",
    "\n",
    "We'll use the Tiny Shakespeare dataset to train the GPT-2 model \n",
    "\n",
    "The first training run just uses the very small training sample of B=4, T=32 and a learning rate of 3e-4 which is ideal for early stages of model training with small samples. The small data set gets reloaded at every training step. This reduces the loss very rapidly with an overfitted set of weights but it demonstrates that the model is learning.\n",
    "\n",
    "```\n",
    "Using device: cuda\n",
    "Initial randomized loss should be about -ln(1/50,257) = 10.825 if all 50,257 tokens are equally likely:\n",
    " tensor(11.1613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
    "step 0, loss: 11.161322593688965\n",
    "step 1, loss: 6.7335896492004395\n",
    "step 2, loss: 4.3205389976501465\n",
    "   :         :           :\n",
    "step 48, loss: 0.0030398822855204344\n",
    "step 49, loss: 0.0029682039748877287\n",
    "```\n",
    "The next stage is to iterate over different batches which requires a dataloader to ensure we always get a fresh batch. This ensurs we optimize over a reasonable data set.\n",
    "\n",
    "#### Fixing a Hugging Face model shared weights \"bug\"\n",
    "The weights from Hugging Face come in with the `lm_head.weight` and `transformer.wte.weight` having the same 2D shape:\n",
    "```\n",
    "print(sd_hf[\"lm_head.weight\"].shape)\n",
    "print(sd_hf[\"transformer.wte.weight\"].shape)\n",
    "\n",
    "torch.Size([50257, 768])\n",
    "torch.Size([50257, 768])\n",
    "```\n",
    "+ `lm_head.weight` is the token embedding at the bottom of the transformer decoder (listed as **Output Embedding** in the **Attention is All you Need** diagram). \n",
    "+ `transformer.wte.weight` is the language model head classifier layer at the top of the transformer (listed as **Linear Layer** at the top of the decoder in the **Attention is All you Need** diagram).\n",
    "\n",
    "Not only do these input and output tensors have the same shape, they have element-wise equality:\n",
    "```\n",
    "(sd_hf[\"lm_head.weight\"] == sd_hf[\"transformer.wte.weight\"]).all()\n",
    "\n",
    "tensor(True)\n",
    "```\n",
    "And going one step further we can also see they have the same PyTorch data pointer which is refered to as a **weight-tying scheme**:\n",
    "```\n",
    "print(sd_hf[\"lm_head.weight\"].data_ptr())\n",
    "print(sd_hf[\"transformer.wte.weight\"].data_ptr())\n",
    "\n",
    "1406505003360576\n",
    "1406505003360576\n",
    "\n",
    "```\n",
    "Having the same PyTorch data pointer means they are pointing to an identical tensor. Two embeddings that are measurably close in the token embedding feeding in to the transformer should also have similar probabilities in the classifier layer at the top of the Transformer.\n",
    "\n",
    "Employing a **weight-tying scheme** is by design in the **Attention is All you Need** paper as it leads to optimization to tie these weights together. \n",
    "\n",
    "You can see this in action in OpenAI's released GPT-2 TensorFlow code (**gpt-2/src/model.py**) in the definition of the **model** class. When forwarding the model, the **wte** matrix which encodes the token embeddings are used when feeding data into the transformer as you would expect but they are also used when calculating the logits at the tail end of the transformer process. **wte** is also used at both ends of the Transformer during backpropogation.\n",
    "\n",
    "All of that is a long way of saying we weren't doing this data sharing so we fixed this \"bug\" by adding this functionality! This was accomplished by adding this line to the GPT class which simply copies the PyTorch data pointer from `self.transformer.wte.weight` to `self.lm_head.weight`:\n",
    "```\n",
    "# Weight sharing/tying\n",
    "self.transformer.wte.weight = self.lm_head.weight\n",
    "```\n",
    "Weight typing results in `wte.weight` being orphaned but PyTorch handles the clean up under the hood.\n",
    "\n",
    "This optimization saves **768*50257** = **38,597,376** parameters which is roughly a third of all of the parameters in the 124M model.\n",
    "\n",
    "#### Model initialization weights\n",
    "\n",
    "How the model was initialized isn't explicitly stated anywhere. However, by again looking at the OpenAI sopurce code for GPT-2 (**gpt-2/src/model.py**) we can see in definition of the **conv1d** class that they use weights initiation with a standard deviation of 0.02:\n",
    "```\n",
    "def conv1d(x, scope,*, w_init_stddev=0.002):\n",
    "```\n",
    "Later in the **model** class code, positional embeddings are initialized with a standard deviation of 0.01 and token embeddings with a initialized with a standard deviation of 0.02.\n",
    "```\n",
    "wte = tf.get_variable('wpe',[hparams.n_ctx, hparams.n_embd], initializer=tf.random_normal_initializer(stddev=0.001))\n",
    "wpe  = tf.get_variable('wpe',[hparams.n_vocab, hparams.n_embd], initializer=tf.random_normal_initializer(stddev=0.002))\n",
    "```\n",
    "We can set up something similar. Typically you would want a value that changes if the model gets larger using something like Xavier initialization. However, we are hard coding because that is what OpenAI did! It turns out that Xavier initialization which is equal to one over the square root of the number of features is very close to 0.02 anyway.\n",
    "\n",
    "#### Monitoring training with std and inspecting torch.no_grad()\n",
    "\n",
    "You can monitor the standard deviation of the output logits to check that the training is stable.\n",
    "```\n",
    "std = logits.std()\n",
    "```\n",
    "When loss is falling steadily the model is learning and when the Standard deviation of logits is rising slightly it suggests that the model's outputs are becoming more confident (moving away from uniform probability across the vocabulary). In the early steps, the model is mostly guessing — so the logits have a low spread. As it learns, the distribution sharpens slightly (but shouldn’t explode), hence the small increase in std.If std starts to rise past 2 or 3 there s a risk of exploding gradients and if it drops below 0.1 there is a risk of vanishing gradients. If there are sudden jumps in standard deviation or loss there is potential numerical instability or bad gradient spikes.\n",
    "\n",
    "Looking at **torch.no_grad()** is a great way to inspect activations or logits without interfering with gradients. Andrej prefers to look at activations and logits, not raw token IDs when monitoring progress.\n",
    "\n",
    "Other useful diagnostics include:\n",
    "+ logits.mean().item()\n",
    "+ logits.max().item()\n",
    "+ logits.min().item()\n",
    "+ grad_norm = model.lm_head.weight.grad.norm().item() - after running `.backward()`\n",
    "\n",
    "These are great for building an intuitive feel for how models behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15bf835a-c988-4ca1-9be6-45d6bce79c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda \n",
      "\n",
      "Batch Size: 16, Time: 768.\n",
      "Loaded 338025 tokens.\n",
      "\n",
      "1 epoch = 27 batches.\n",
      "\n",
      "Weight tying\n",
      "\n",
      "Initialize weights with standard deviation 0.02\n",
      "\n",
      "Initial randomized loss should be about -ln(1/50,257) = 10.825 if all 50,257 tokens are equally likely:\n",
      "tensor(10.9487, device='cuda:0', grad_fn=<NllLossBackward0>) \n",
      "\n",
      "step 0, loss: 10.949,  dt: 503.94ms,  tok/s 24384.05,  std: 0.55\n",
      "step 1, loss: 9.408,  dt: 231.98ms,  tok/s 52970.83,  std: 0.55\n",
      "step 2, loss: 8.829,  dt: 229.82ms,  tok/s 53469.09,  std: 0.56\n",
      "step 3, loss: 8.719,  dt: 225.74ms,  tok/s 54434.72,  std: 0.56\n",
      "step 4, loss: 8.602,  dt: 221.76ms,  tok/s 55410.65,  std: 0.57\n",
      "step 5, loss: 8.286,  dt: 225.97ms,  tok/s 54378.78,  std: 0.59\n",
      "step 6, loss: 8.227,  dt: 219.59ms,  tok/s 55960.06,  std: 0.60\n",
      "step 7, loss: 8.055,  dt: 223.09ms,  tok/s 55081.57,  std: 0.62\n",
      "step 8, loss: 7.922,  dt: 221.15ms,  tok/s 55564.18,  std: 0.66\n",
      "step 9, loss: 7.742,  dt: 223.86ms,  tok/s 54891.27,  std: 0.69\n",
      "step 10, loss: 7.441,  dt: 221.45ms,  tok/s 55489.40,  std: 0.72\n",
      "step 11, loss: 7.285,  dt: 234.13ms,  tok/s 52482.72,  std: 0.76\n",
      "step 12, loss: 7.124,  dt: 228.60ms,  tok/s 53752.27,  std: 0.79\n",
      "step 13, loss: 6.931,  dt: 220.41ms,  tok/s 55751.34,  std: 0.85\n",
      "step 14, loss: 6.821,  dt: 226.05ms,  tok/s 54358.88,  std: 0.89\n",
      "step 15, loss: 6.717,  dt: 222.48ms,  tok/s 55233.16,  std: 0.93\n",
      "step 16, loss: 6.611,  dt: 227.85ms,  tok/s 53931.13,  std: 0.97\n",
      "step 17, loss: 6.657,  dt: 227.34ms,  tok/s 54052.06,  std: 1.01\n",
      "step 18, loss: 6.662,  dt: 230.16ms,  tok/s 53389.78,  std: 1.04\n",
      "step 19, loss: 6.627,  dt: 225.87ms,  tok/s 54402.14,  std: 1.07\n",
      "step 20, loss: 6.564,  dt: 223.67ms,  tok/s 54937.37,  std: 1.11\n",
      "step 21, loss: 6.392,  dt: 223.21ms,  tok/s 55050.98,  std: 1.14\n",
      "step 22, loss: 6.668,  dt: 226.72ms,  tok/s 54198.65,  std: 1.17\n",
      "step 23, loss: 6.484,  dt: 226.37ms,  tok/s 54283.59,  std: 1.20\n",
      "step 24, loss: 6.504,  dt: 230.87ms,  tok/s 53225.47,  std: 1.23\n",
      "step 25, loss: 6.433,  dt: 223.60ms,  tok/s 54955.77,  std: 1.25\n",
      "step 26, loss: 6.476,  dt: 225.77ms,  tok/s 54426.96,  std: 1.27\n",
      "step 27, loss: 6.357,  dt: 220.41ms,  tok/s 55749.59,  std: 1.28\n",
      "step 28, loss: 6.246,  dt: 226.61ms,  tok/s 54224.42,  std: 1.30\n",
      "step 29, loss: 6.145,  dt: 223.30ms,  tok/s 55028.17,  std: 1.32\n",
      "step 30, loss: 6.251,  dt: 220.85ms,  tok/s 55639.40,  std: 1.34\n",
      "step 31, loss: 6.205,  dt: 223.75ms,  tok/s 54919.34,  std: 1.36\n",
      "step 32, loss: 6.069,  dt: 224.90ms,  tok/s 54636.92,  std: 1.38\n",
      "step 33, loss: 6.299,  dt: 231.75ms,  tok/s 53022.00,  std: 1.39\n",
      "step 34, loss: 6.364,  dt: 227.79ms,  tok/s 53944.62,  std: 1.41\n",
      "step 35, loss: 6.467,  dt: 222.25ms,  tok/s 55289.69,  std: 1.41\n",
      "step 36, loss: 6.430,  dt: 224.63ms,  tok/s 54702.74,  std: 1.41\n",
      "step 37, loss: 6.201,  dt: 223.75ms,  tok/s 54917.59,  std: 1.42\n",
      "step 38, loss: 6.259,  dt: 223.76ms,  tok/s 54914.90,  std: 1.43\n",
      "step 39, loss: 6.184,  dt: 220.28ms,  tok/s 55784.23,  std: 1.43\n",
      "step 40, loss: 6.154,  dt: 224.29ms,  tok/s 54786.82,  std: 1.44\n",
      "step 41, loss: 6.119,  dt: 226.21ms,  tok/s 54320.20,  std: 1.46\n",
      "step 42, loss: 6.086,  dt: 222.27ms,  tok/s 55284.17,  std: 1.48\n",
      "step 43, loss: 6.072,  dt: 225.80ms,  tok/s 54419.83,  std: 1.48\n",
      "step 44, loss: 6.219,  dt: 221.57ms,  tok/s 55458.59,  std: 1.50\n",
      "step 45, loss: 6.295,  dt: 223.78ms,  tok/s 54911.33,  std: 1.52\n",
      "step 46, loss: 6.282,  dt: 225.87ms,  tok/s 54404.03,  std: 1.55\n",
      "step 47, loss: 6.227,  dt: 229.99ms,  tok/s 53429.02,  std: 1.57\n",
      "step 48, loss: 6.047,  dt: 223.78ms,  tok/s 54910.51,  std: 1.58\n",
      "step 49, loss: 6.240,  dt: 235.32ms,  tok/s 52218.18,  std: 1.58\n",
      "\n",
      "Total training time: 11.56ms\n",
      "\n",
      "Check data types for precision (bfloat16 is optimal)\n",
      "Logits data type: torch.bfloat16\n",
      "Transformer wte matrix data type: torch.float32\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import tiktoken\n",
    "import torch._dynamo as dynamo\n",
    "\n",
    "# Build dedicated DataLoader class hat suppports stepping thrtough the entuire data set encoding\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        file_path = r\"07 - Let's Build GPT (nanoGPT)\\input.txt\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:                # Load full text from disk\n",
    "            text = f.read()\n",
    "        \n",
    "        # Just keep these small data samples on the CPU to free up space on GPU\n",
    "        enc = tiktoken.get_encoding('gpt2')                              # Store entire encoding in memory at initialization\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f\"Loaded {len(self.tokens)} tokens.\\n\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches.\\n\")     # How many batches are needed for each epoch?\n",
    "\n",
    "        self.current_position = 0                                        # Start at position 0 in encoding\n",
    "\n",
    "    # Advance through encoding in groups of B*T but making sure to always grab the extra token needed for final target\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "\n",
    "        buf = self.tokens[self.current_position:self.current_position+(B*T)+1]\n",
    "        x = (buf[:-1]).view(B, T)                                        # Inputs\n",
    "        y = (buf[1:]).view(B, T)                                         # Targets\n",
    "\n",
    "        self.current_position += B * T                                   # Advance the position in the tensor \n",
    "\n",
    "        if self.current_position + (B*T)+1 > len(self.tokens):           # If we reach the end of the encoding ...\n",
    "            self.current_position = 0                                    # Loop back round and start again\n",
    "        return x, y\n",
    "    \n",
    "# Autodetect available devices to automatically run on the device with highest capability.\n",
    "# Ideally run on a GPU with CUDA support if one is available\n",
    "device = \"cpu\"                                            # By default select cpu which is always available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"                                       # Switch to best option cuda if it exists\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"                                        # Try mps if cuda not available (mps is on decent Apple macbooks)\n",
    "print(f\"Using device: {device}\", \"\\n\")\n",
    "#device = \"cpu\"                                           # OVERRIDE to run on CPU by uncommenting\n",
    "\n",
    "# Hyperparameters\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "# Set seeds for reproducability\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# Original dataloader for Tiny Shakespeare Dataset and encoder -- REPLACED BY DEDICATED DATALOADER ABOVE\n",
    "#enc = tiktoken.get_encoding('gpt2')\n",
    "#file_path = r\"07 - Let's Build GPT (nanoGPT)\\input.txt\"\n",
    "#with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#    text = f.read()\n",
    "#text = text[:1000]                                              # First 1,000 characters (which is roughly 300 tokens)\n",
    "#tokens = enc.encode(data)                                       # Encode the data we extracted from the text earlier\n",
    "#                                                                \n",
    "#B, T = 4, 32                                                    # Batch and Time gives size of input and table tensors\n",
    "#buf = torch.tensor(tokens[:B*T + 1])                            # Include extra token to have ground truth for final label\n",
    "#buf = buf.to(device)                                            # Move buf to GPU if available. Note this requires \"buf =\"\"\n",
    "#x = buf[:-1].view(B, T)                                         # .view gives 2D rearrangement of tokens\n",
    "#y = buf[1:].view(B, T)                                          # y loads all but first token (x loaded all but last)\n",
    "\n",
    "# Clear cache if starting affresh\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Call dedicated dataloader\n",
    "B = 16                                                           # 16  Batch size of 24 gives out of memory error on RTX3090\n",
    "T = 768                                                          # 768 T=1024 runs but is relatively slow\n",
    "print(f\"Batch Size: {B}, Time: {T}.\")\n",
    "train_loader = DataLoaderLite(B=B, T=T)\n",
    "    \n",
    "# Setting this will use TensorFloat32 precision if its available which is a more optimal matrix multiplication method\n",
    "torch.set_float32_matmul_precision('high')                       # 'Highest' is default for float32, 'High' is tensor32\n",
    "\n",
    "# Create model\n",
    "model = GPT(GPTConfig(vocab_size=50304))                         # Create larger vocab size that is nice multiple of 2\n",
    "model.to(device)\n",
    "\n",
    "# Torch.compile is a Linux just-in-time compiler which is added here for completeness. It must be run in \"eager\" mode to have\n",
    "# any chance of running on Windows. Even then, it may not speed things up much and may actually slow things down. However, it\n",
    "# is added here for completeness and testing. If it doesn't help the execution times much on Windows disable it!\n",
    "# backend=\"aot_eager\" is same as \"eager\" but routes through AOTAutograd which is useful for debugging trace errors.\n",
    "# torch.complie requires installation of functorch. \n",
    "#print(\"Compiling with torch.complile...\")\n",
    "#model = torch.compile(model, backend=\"aot_eager\")               # Compile neural network model for faster execution (Linix)\n",
    "#print(\"Complete\\n\")\n",
    "\n",
    "# Optimize! - Train the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)       # AdamW optimizer with 2 moment buffers (Momentum, RMSProp)\n",
    "tstart = time.time()                                             # Record start time of training loop\n",
    "for i in range(50):                                              # 50 training steps\n",
    "    t0 = time.time()                                             # Set timer start\n",
    "    x, y = train_loader.next_batch()                             # Walk through the entire encoding by grabbing next batch\n",
    "    x, y = x.to(device), y.to(device)                            # Place tensors on GPU   \n",
    "    optimizer.zero_grad()                                        # Always remember to start with zero gradients!\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16): # Invoke bfloat16 autocasting to optimize data flow ...\n",
    "        logits, loss = model(x, y)                               # only available on Ampere GPUs (which includes RTX3090)\n",
    "    loss.backward()                                              # Accumulate gradients from loss\n",
    "    optimizer.step()                                             # Update parameters to decrease the loss\n",
    "    std = logits.std()                                           # Monitor standard deviation of logits to check stability\n",
    "    if (i==0):\n",
    "        print(\"Initial randomized loss should be about -ln(1/50,257) = 10.825 if all 50,257 tokens are equally likely:\")\n",
    "        print(loss, \"\\n\")\n",
    "    torch.cuda.synchronize()                                     # Complete any work in queue so time() is more accurate\n",
    "    t1 = time.time()                                             # Set timer end\n",
    "    dt = (t1 - t0)*1000                                          # Time difference in milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1-t0)\n",
    "    # Print stats for each training step. Item converts tensor to float and moves from GPU to CPU\n",
    "    print(f\"step {i}, loss: {loss.item():.3f},  dt: {dt:.2f}ms,  tok/s {tokens_per_sec:.2f},  std: {std:.2f}\")          \n",
    "\n",
    "tend = time.time()                                                # Set timer for end of trainiong loop\n",
    "print(f\"\\nTotal training time: {tend-tstart:.2f}ms\")\n",
    "print(\"\\nCheck data types for precision (bfloat16 is optimal)\")\n",
    "print(\"Logits data type:\", logits.dtype)\n",
    "print(\"Transformer wte matrix data type:\", model.transformer.wte.weight.dtype)\n",
    "\n",
    "sys.exit(0)                                                      # Early exit\n",
    "\n",
    "# Generate right now x is (B, T) where B=5 and T=8. \n",
    "#torch.manual_seed(42)                                            # Set seed to 42\n",
    "#torch.cuda.manual_seed(42)                                       # Match seed on GPU\n",
    "while x.size(1) < max_length:\n",
    "    # Forward thge model to get the logits\n",
    "    with torch.no_grad():                                        # no_grad means don't call backpropagation so less processing\n",
    "        logits = model(x)                                        # (B, T, vocab_size)\n",
    "        logits = logits[:, -1, :]                                # (B, vocab_size) - logits at the last position\n",
    "        probs = F.softmax(logits, dim=-1)                        # Get the probabilities\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1) # top-k sampling of 50 - topk_probs/indices become (5, 50)\n",
    "        ix = torch.multinomial(topk_probs,1)                     # (B, 1) - Select a token from the top-k probabilities\n",
    "        xcol = torch.gather(topk_indices, -1, ix)                # (B, 1) - Gather corresponding indices\n",
    "        x = torch.cat((x, xcol), dim=1)                          # Append thr sequence\n",
    "    \n",
    "# Print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()                          # Get the tokens from all of the 5 samples \n",
    "    decoded = enc.decode(tokens)                                 # Use tiktoken's decoder to convert tokens back to characters\n",
    "    print(\">\", decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d74a22-fa5d-42c9-befd-d5e3d7cc9c92",
   "metadata": {},
   "source": [
    "## Optimization tricks\n",
    "\n",
    "#### Making best use of your GPU\n",
    "\n",
    "Running `C:\\Windows\\System32\\nvidia-smi.exe` in a Power Shell will give you lots of useful information about your GPU which you can use to configure how you run large Neural Net models efficiently. When Andrej ran the command it showed he had eight A100 Tensor Core GPUs each with 80GB of memory. That's over $100,000 worth of advanced graphics cards!! I just have a single GPU. But it is an RTX3090 so that's not so bad! Andrej's configuration comes from hiring a powerful machine at Lambda labs which I could do for the final training step if needed.\n",
    "```\n",
    "PS C:\\Users\\johns> C:\\Windows\\System32\\nvidia-smi.exe\n",
    "Sun Aug  3 16:55:58 2025\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 577.00                 Driver Version: 577.00         CUDA Version: 12.9     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3090      WDDM  |   00000000:0A:00.0  On |                  N/A |\n",
    "|  0%   54C    P8             37W /  420W |    1084MiB /  24576MiB |      6%      Default |\n",
    "|                                         |                        |                  N/A |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "```\n",
    "#### Using bfloat16 or tensorfloat32 where possible\n",
    "By default Tensors are often created as float32 which can much more than is needed. You can make large networks much more efficient by reducing the number of bits in the tensors which lowers the precision of the model at very little cost in accuracy overall. Instead of using FP32 consider using TF32 (TensorFloat 32) which is 8 times faster. \n",
    "\n",
    "<img src=\"../assets/TensorFlow32-format.jpg\" alt=\"TensorFlow32 format\" style=\"width: 600px;\"/>\n",
    "\n",
    "There are more details in the A100 Architecture White Paper. One key feature of TensorFLoats is that is that they crop the mantissa in interal operations which makes them run much faster at the cost of a litte pricison which is often not noticable in prtactice. They still return a standard float 32 so code can run on them without requiring any modification at all.\n",
    "\n",
    "<img src=\"../assets/Tensor-cropped-mantissa.jpg\" alt=\"Tensor with cropped mantissa\" style=\"width: 600px;\"/>\n",
    "\n",
    "Of course, this is only useful if you have Tensor Cores or are hiring a machine that has them!\n",
    "\n",
    "The next level down in precision is using a bfloat16 which is still very suitable for out needs but much faster to computer - and more importantly also much faster to move data around to reduce bottlenecks when waiting for data to move back and forth. Float16 is less useful as it does not represent the full exponent range that you get with bfloat16 because it uses more of its bits for mantissa precision accuracy. When float16 is used gradient scaling workarounds are required to deal with the loss of exponent range which adds significantly to the complexity of model building. bfloat16 does not require these workarounds with a trade off of slightly reduced precision which is often not significant. \n",
    "\n",
    "You can read about dealing with precision in the \"Atomatic Mixed Precision\" PyTorch document by Micheal Carilli. Most of the document is not relevant but the section on torch.autcast is useful. This section says to not set bfloat16 explicitly but to let PyTOrchj handle using it when appropriate using autocasting. It also says to only use on the forward pass not in backpropogation steps. This is the line added to the code to make use of it:\n",
    "```\n",
    "with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "    logits, loss = model(x, y)\n",
    "```\n",
    "Note how this line is within the optimizer loop which is a forward pass only.\n",
    "\n",
    "#### torch.compile\n",
    "\n",
    "This is a mechanism for compiling neural nets so they run faster. It's just a single line of code to implement:\n",
    "```\n",
    "model = torch.compile(model) \n",
    "```\n",
    "This will have an intitial slow down while the model is compiled but will give a speed up during training which can make a huge difference for long training runs of big models.\n",
    "\n",
    "torch.complie requires installation of functorch. In a conda shell you can run:\n",
    "```\n",
    "conda install -c pytorch functorch\n",
    "```\n",
    "Torch compile using dymamo is designed for Linux. Although there are Windows implementations for the required triton mpdule they are very unstable and may cause CUDA problems. You can still run torch_compile in Windows but you must run it in **eager** mode. This will disable the Inductor to run it without Triton:\n",
    "```\n",
    "model = torch.compile(model, backend=\"eager\")\n",
    "```\n",
    "It won’t give you speedups (and may slow things down a tiny bit), but it keeps your code future-proof and testable. Ideally, switching to a Linux environment in WSL2, Docker, or Ubuntu is the safe way to implement torch.compile\n",
    "\n",
    "Really, torch.compile should be run by default if you have support for it unless you are debugging so need to run lots of loops really fast. It should reduce each training step to less than half of its execution time.\n",
    "\n",
    "#### Flash attention\n",
    "\n",
    "Flash attention is an efficient way to run attention by fusing four separate operations into one. It actually does more calculations but is still faster because the extra computations are fast and are designed to reduce the read and writes which are very slow.\n",
    "\n",
    "#### Fix ugly numbers!\n",
    "This is a suprisingly effective optimization that simply tweaks any numbers that aren't nice multiples of 2. A great example is the number of tokens can be increased from 50257 to 50304. Although this increases the total computations required it removes the need for special handling as the number can eqasily be broken down in to multiples of 8,16, 32, 64 and 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43ccba-6d10-48e0-bbb5-a39ff0ef97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check torch.compile (experimental)\n",
    "import torch._dynamo as dynamo\n",
    "dynamo.explain(model)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544967b9-6750-45a7-a0f4-948959e394ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Strategies for reducing overfitting\n",
    "Validation Strategy Proposal for GPT-2 Training Using FineWeb-edu\n",
    "\n",
    "#### Current Setup Recap\n",
    "+ Model: GPT-2 (custom implementation following Karpathy)\n",
    "+ Dataset: FineWeb-edu 10B token sample\n",
    "+ Validation split: 1 shard out of 100 (1%)\n",
    "+ Validation pattern: Scanned linearly with no shuffling\n",
    "+ Observation: Training loss continues to fall; validation loss decreases initially, then steadily increases from ~step 3500 to step 16000.\n",
    "\n",
    "#### Concerns Identified\n",
    "+ Small validation set (1%) may be:\n",
    "    + Too narrow to reflect generalization ability\n",
    "    + Susceptible to variance or domain mismatch\n",
    "    + Missing harder or more diverse examples\n",
    "+ No corpus randomization may introduce positional bias and smooth overfitting\n",
    "\n",
    "#### Proposed Adjustments\n",
    "Option 1: Expand Validation Set Using Additional FineWeb-edu Shards\n",
    "+ Identify additional FineWeb shards not included in the current 10B token sample.\n",
    "+ Sample a fixed number of examples (e.g., 2% to 5% of total corpus) from different files.\n",
    "+ Ensure domain and difficulty diversity (e.g., include harder educational material).\n",
    "+ Pre-tokenize and cache if necessary.\n",
    "\n",
    "Option 2: Stratify Validation Selection\n",
    "+ Perform lightweight metadata classification or domain tagging of samples.\n",
    "+ Use stratified sampling to create a validation set that matches the full corpus distribution.\n",
    "+ May require one pass through the corpus but would give more reliable results.\n",
    "\n",
    "Option 3: Rotate Validation Across Epochs\n",
    "+ Instead of a fixed validation shard, rotate which shard is held out each epoch.\n",
    "+ Compute running average of validation loss.\n",
    "+ Benefit: exposes model to broader set of out-of-training samples.\n",
    "\n",
    "Option 4: Hybrid: Keep Static + Floating Validation Sets\n",
    "+ Maintain original shard as static benchmark.\n",
    "+ Add rotating or diversified secondary validation set to compare trends.\n",
    "+ Track both sets during training and compare convergence behavior.\n",
    "\n",
    "#### Tooling Enhancements\n",
    "+ Track validation loss with higher frequency (e.g., every 200 steps) to catch sharper inflections.\n",
    "+ Store per-validation checkpoint metrics to assess per-shard behavior.\n",
    "\n",
    "#### Next Steps\n",
    "1. Identify and acquire additional FineWeb-edu shards.\n",
    "2. Analyze overlap (if any) with training shards to ensure clean separation.\n",
    "3. Implement at least one of the proposed strategies and replot training vs. validation.\n",
    "\n",
    "#### Goal\n",
    "Make validation loss a more meaningful diagnostic of generalization, and ensure training signals guide architecture/tuning decisions effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpt-lab)",
   "language": "python",
   "name": "gpt-lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
